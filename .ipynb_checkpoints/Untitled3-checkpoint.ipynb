{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059e68ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddyrga/opt/miniconda3/envs/tensorflow69/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from random import shuffle as random_shuffle\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.applications import vgg16, inception_v3, resnet\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import os\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from vit_keras import vit\n",
    "import numpy as np\n",
    "import click\n",
    "from numpy.random import seed as np_seed\n",
    "from random import seed\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from load import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e5b35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import click\n",
    "from numpy.random import seed as np_seed\n",
    "from random import seed\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cb9e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xs_ys(ill_positive, negatives, index):\n",
    "    neg = negatives[index]\n",
    "    xs = np.concatenate((ill_positive[index], neg[:len(ill_positive[index])]))\n",
    "    ys = np.array(len(ill_positive[index]) * [1] + len(ill_positive[index]) * [0])\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcbca260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepr(img_arr, img_size=75):\n",
    "    if img_arr.shape[-2] == img_size:\n",
    "        return preprocess_input(img_arr)\n",
    "    else:\n",
    "        return cv2.resize(preprocess_input(img_arr), dsize=(img_size, img_size), interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70b78ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patient(path, img_size=75):\n",
    "    imgs = []\n",
    "    imgs_arr = np.load(path, allow_pickle=True)\n",
    "    for img_arr in imgs_arr:\n",
    "        imgs.append(prepr(img_arr, img_size))\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd473422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medtransfer_data_npy(path, i, img_size=75):\n",
    "    ill_path = os.path.join(path, 'ill/npy')\n",
    "    ill_positive = []\n",
    "    ill_negative = []\n",
    "\n",
    "    for file in sorted(os.listdir(ill_path)):\n",
    "        if 'pos' in file:\n",
    "            ill_positive.append(load_patient(os.path.join(ill_path, file), img_size))\n",
    "        else:\n",
    "            ill_negative.append(load_patient(os.path.join(ill_path, file), img_size))\n",
    "\n",
    "    healthy_path = os.path.join(path, 'healthy/npy')\n",
    "    healthy_negative = []\n",
    "    for file in sorted(os.listdir(healthy_path)):\n",
    "        healthy_negative.append(load_patient(os.path.join(healthy_path, file), img_size))\n",
    "\n",
    "    negatives = []\n",
    "    for ill, healthy in zip(ill_negative, healthy_negative):\n",
    "        negatives.append(np.concatenate((ill, healthy)))\n",
    "\n",
    "    test_xs, test_ys = extract_xs_ys(ill_positive, negatives, -i)\n",
    "    validation_xs, validation_ys = extract_xs_ys(ill_positive, negatives, -i-1)\n",
    "    train_positive = np.concatenate(ill_positive[:-i-2] + ill_positive[-i:])\n",
    "    train_negative = np.concatenate(negatives[:-i-2] + negatives[-i:])\n",
    "\n",
    "    np.random.seed(0)\n",
    "    random_shuffle(train_positive)\n",
    "    random_shuffle(train_negative)\n",
    "\n",
    "    len_train_pos = len(train_positive)\n",
    "    train_negative = train_negative[:len_train_pos]\n",
    "\n",
    "\n",
    "    train = [x for x in zip(np.concatenate((train_positive, train_negative)), [1] * len_train_pos + [0] * len_train_pos)]\n",
    "    random_shuffle(train)\n",
    "    train_xs, train_ys = list(zip(*train))\n",
    "\n",
    "    return np.array(train_xs), np.array(train_ys), validation_xs, validation_ys, test_xs, test_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07f5a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(base_model_name, image_size, num_classes, activation='linear',\n",
    "                      include_top=True, pretrained=True, pretrained_top=True,\n",
    "                      weights=\"imagenet21k+imagenet2012\"):\n",
    "    if base_model_name == 'ViT_l16':\n",
    "        base_model = vit.vit_l16(image_size=image_size, classes=num_classes, activation=activation,\n",
    "                                 include_top=include_top, pretrained=pretrained,\n",
    "                                 pretrained_top=pretrained_top)\n",
    "    elif base_model_name == 'ViT_b16':\n",
    "        base_model = vit.vit_b16(image_size=image_size, classes=num_classes, activation=activation,\n",
    "                                 include_top=include_top, pretrained=pretrained,\n",
    "                                 pretrained_top=pretrained_top)\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect model name passed to transformer_model\")\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15ad6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediatemodel(base_model, output_index_layer, unfreeze=False, unfreeze_from=0):\n",
    "    intermediate_layer_model = Model(inputs=base_model.input,\n",
    "                                     outputs=base_model.get_layer(index=output_index_layer).output)\n",
    "\n",
    "    i = 0\n",
    "    for layer in intermediate_layer_model.layers:\n",
    "        i += 1\n",
    "        layer.trainable = False\n",
    "\n",
    "    if unfreeze:\n",
    "        for layer in intermediate_layer_model.layers[unfreeze_from:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    return intermediate_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e094ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionmodel(base_model, dense_sizes=[128], dense_activations=['relu'],\n",
    "                    dense_kernel_inits=['random_uniform'], dropout_p=[0.5],\n",
    "                    pooling='avg', classes=1,\n",
    "                    class_activation='sigmoid'):\n",
    "    x = base_model.output\n",
    "    if pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif pooling == 'max':\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    elif pooling == 'extract_token':  # Used with Vision Transformer Cut\n",
    "        x = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"Transformer/encoder_norm\")(x[0])\n",
    "        x = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(x)\n",
    "    elif pooling is None:\n",
    "        pass  # Pooling is not needed for Vision Transformer\n",
    "    else:\n",
    "        x = Flatten(name=\"flatten\")(x)\n",
    "\n",
    "    for i, (ds, da, dki, dop) in enumerate(zip(dense_sizes, dense_activations, dense_kernel_inits, dropout_p)):\n",
    "        x = Dense(ds, activation=da, kernel_initializer=dki)(x)\n",
    "        name = 'Dropout_Regularization_' + str(i)\n",
    "        x = Dropout(dop, name=name)(x)\n",
    "\n",
    "    predictions = Dense(classes, activation=class_activation, name='Output')(x)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbe4c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_dir, experiment_name, architecture_type, dataset, epochs, patience, seeds_nmb, folds_nmb, save_test_data,\n",
    "        sam_model):\n",
    "    \n",
    "    \n",
    "    batch_size = 100\n",
    "    dest_img_size = 64\n",
    "\n",
    "\n",
    "    for i in range(folds_nmb):\n",
    "        print(f'TRAINING {i} model...')\n",
    "            \n",
    "        train_xs, train_ys, validation_xs, validation_ys, test_xs, test_ys = load_medtransfer_data_npy(data_dir, i, dest_img_size)\n",
    "\n",
    "        if save_test_data:\n",
    "            np.save(f'{experiment_name}_{i}_test_xs.npy', test_xs)\n",
    "            np.save(f'{experiment_name}_{i}_test_ys.npy', test_ys)\n",
    "                \n",
    "        base_model = transformer_model('ViT_b16', image_size=64, num_classes=1,\n",
    "                                             pretrained_top=False, activation='sigmoid', include_top=False)\n",
    "        print(base_model.parameters())\n",
    "        \n",
    "#         midmodel = intermediatemodel(base_model, 10)\n",
    "\n",
    "#         headmodel = predictionmodel(midmodel, pooling='extract_token')\n",
    "            \n",
    "#         model = Model(inputs=midmodel.input, outputs=headmodel)\n",
    "#         if sam_model:\n",
    "#             base_model_input_shape = model.input_shape\n",
    "#             model = SharpnessAwareMinimization(model)\n",
    "#             model.build(input_shape=base_model_input_shape)\n",
    "            \n",
    "        break\n",
    "\n",
    "#         reduce_lr = ReduceLROnPlateau(\n",
    "#             mode='min',\n",
    "#             monitor='val_loss',\n",
    "#             factor=0.1,\n",
    "#             min_lr=5e-7,\n",
    "#             patience=10,\n",
    "#             verbose=1)\n",
    "\n",
    "#         es = EarlyStopping(\n",
    "#             monitor=\"val_loss\",\n",
    "#             patience=patience,\n",
    "#             mode=\"min\",\n",
    "#             restore_best_weights=True,\n",
    "#             verbose=1)\n",
    "\n",
    "#         opt = SGD(learning_rate=0.001, momentum=0.9, name=\"SGD\")\n",
    "\n",
    "#         model.compile(opt,loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "#         model.fit(x=train_xs,\n",
    "#                   y=train_ys,\n",
    "#                   validation_data=(validation_xs, validation_ys),\n",
    "#                   batch_size=batch_size,\n",
    "#                   epochs=epochs,\n",
    "#                   callbacks=[reduce_lr, es])\n",
    "\n",
    "#         stopped_epoch = es.stopped_epoch\n",
    "#         if not sam_model:\n",
    "#             model.save(f'model-{experiment_name}_{i}_epoch{stopped_epoch}.h5')\n",
    "#         else:\n",
    "#                 # Custom models are not serializable\n",
    "#             model.save(f'model-{experiment_name}_{i}_epoch{stopped_epoch}', save_format='tf')\n",
    "#         preds = model.predict(test_xs, verbose=1)\n",
    "#         print()\n",
    "#         print([round(float(x), 3) for x in preds])\n",
    "#         preds = preds > 0.5\n",
    "#         print()\n",
    "#         print([int(x) for x in preds])\n",
    "\n",
    "#         print()\n",
    "#         results = [accuracy_score(preds, test_ys),\n",
    "#                    f1_score(preds, test_ys),\n",
    "#                    precision_score(preds, test_ys),\n",
    "#                    recall_score(preds, test_ys)]\n",
    "#         print(f'ACC: {results[0]}')\n",
    "#         print(f'F1: {results[1]}')\n",
    "#         print(f'PREC: {results[2]}')\n",
    "#         print(f'REC: {results[3]}')\n",
    "#         print()\n",
    "#         print('&' * 50)\n",
    "#         print()\n",
    "\n",
    "#         with open(f'results-{experiment_name}.txt', 'a') as f:\n",
    "#             f.write(str(results))\n",
    "\n",
    "#         i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33a69e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING 0 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddyrga/opt/miniconda3/envs/tensorflow69/lib/python3.8/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 4, 4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/PATIENTS-DATA-ratio-0.5/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitecture_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mViT-CFE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedtransfer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m   \u001b[49m\u001b[43msam_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mseeds_nmb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds_nmb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_test_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [41], line 30\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(data_dir, experiment_name, architecture_type, dataset, epochs, patience, seeds_nmb, folds_nmb, save_test_data, sam_model)\u001b[0m\n\u001b[1;32m     28\u001b[0m     model \u001b[38;5;241m=\u001b[39m SharpnessAwareMinimization(model)\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m.\u001b[39mbuild(input_shape\u001b[38;5;241m=\u001b[39mbase_model_input_shape)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m())    \n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "run(data_dir='data/PATIENTS-DATA-ratio-0.5/', architecture_type='ViT-CFE', dataset='medtransfer', epochs=50, patience=5,\n",
    "   sam_model=False,seeds_nmb=7, folds_nmb=7, save_test_data=False, experiment_name=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "from keras.engine import data_adapter\n",
    "from keras.layers import deserialize as deserialize_layer\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "# isort: off\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "# @generic_utils.register_keras_serializable()\n",
    "# @keras_export(\"keras.models.experimental.SharpnessAwareMinimization\", v1=[])\n",
    "class SharpnessAwareMinimization(Model):\n",
    "    \"\"\"Sharpness aware minimization (SAM) training flow.\n",
    "\n",
    "    Sharpness-aware minimization (SAM) is a technique that improves the model\n",
    "    generalization and provides robustness to label noise. Mini-batch splitting\n",
    "    is proven to improve the SAM's performance, so users can control how mini\n",
    "    batches are split via setting the `num_batch_splits` argument.\n",
    "\n",
    "    Args:\n",
    "      model: `tf.keras.Model` instance. The inner model that does the\n",
    "        forward-backward pass.\n",
    "      rho: float, defaults to 0.05. The gradients scaling factor.\n",
    "      num_batch_splits: int, defaults to None. The number of mini batches to\n",
    "        split into from each data batch. If None, batches are not split into\n",
    "        sub-batches.\n",
    "      name: string, defaults to None. The name of the SAM model.\n",
    "\n",
    "    Reference:\n",
    "      [Pierre Foret et al., 2020](https://arxiv.org/abs/2010.01412)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, rho=0.05, num_batch_splits=None, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.model = model\n",
    "        self.rho = rho\n",
    "        self.num_batch_splits = num_batch_splits\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"The logic of one SAM training step.\n",
    "\n",
    "        Args:\n",
    "          data: A nested structure of `Tensor`s. It should be of structure\n",
    "            (x, y, sample_weight) or (x, y).\n",
    "\n",
    "        Returns:\n",
    "          A dict mapping metric names to running average values.\n",
    "        \"\"\"\n",
    "        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "        if self.num_batch_splits is not None:\n",
    "            x_split = tf.split(x, self.num_batch_splits)\n",
    "            y_split = tf.split(y, self.num_batch_splits)\n",
    "        else:\n",
    "            x_split = [x]\n",
    "            y_split = [y]\n",
    "\n",
    "        gradients_all_batches = []\n",
    "        pred_all_batches = []\n",
    "        for x_batch, y_batch in zip(x_split, y_split):\n",
    "            epsilon_w_cache = []\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = self.model(x_batch)\n",
    "                loss = self.compiled_loss(y_batch, pred)\n",
    "            pred_all_batches.append(pred)\n",
    "            trainable_variables = self.model.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "            gradients_order2_norm = self._gradients_order2_norm(gradients)\n",
    "            scale = self.rho / (gradients_order2_norm + 1e-12)\n",
    "\n",
    "            for gradient, variable in zip(gradients, trainable_variables):\n",
    "                epsilon_w = gradient * scale\n",
    "                self._distributed_apply_epsilon_w(\n",
    "                    variable, epsilon_w, tf.distribute.get_strategy()\n",
    "                )\n",
    "                epsilon_w_cache.append(epsilon_w)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = self(x_batch)\n",
    "                loss = self.compiled_loss(y_batch, pred)\n",
    "            gradients = tape.gradient(loss, trainable_variables)\n",
    "            if len(gradients_all_batches) == 0:\n",
    "                for gradient in gradients:\n",
    "                    gradients_all_batches.append([gradient])\n",
    "            else:\n",
    "                for gradient, gradient_all_batches in zip(\n",
    "                    gradients, gradients_all_batches\n",
    "                ):\n",
    "                    gradient_all_batches.append(gradient)\n",
    "            for variable, epsilon_w in zip(\n",
    "                trainable_variables, epsilon_w_cache\n",
    "            ):\n",
    "                # Restore the variable to its original value before\n",
    "                # `apply_gradients()`.\n",
    "                self._distributed_apply_epsilon_w(\n",
    "                    variable, -epsilon_w, tf.distribute.get_strategy()\n",
    "                )\n",
    "\n",
    "        gradients = []\n",
    "        for gradient_all_batches in gradients_all_batches:\n",
    "            gradients.append(tf.reduce_sum(gradient_all_batches, axis=0))\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        pred = tf.concat(pred_all_batches, axis=0)\n",
    "        self.compiled_metrics.update_state(y, pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of SAM.\n",
    "\n",
    "        SAM delegates the forward pass call to the wrapped model.\n",
    "\n",
    "        Args:\n",
    "          inputs: Tensor. The model inputs.\n",
    "\n",
    "        Returns:\n",
    "          A Tensor, the outputs of the wrapped model for given `inputs`.\n",
    "        \"\"\"\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"model\": generic_utils.serialize_keras_object(self.model),\n",
    "                \"rho\": self.rho,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, custom_objects=None):\n",
    "        # Avoid mutating the input dict.\n",
    "        config = copy.deepcopy(config)\n",
    "        model = deserialize_layer(\n",
    "            config.pop(\"model\"), custom_objects=custom_objects\n",
    "        )\n",
    "        config[\"model\"] = model\n",
    "        return super().from_config(config, custom_objects)\n",
    "\n",
    "    def _distributed_apply_epsilon_w(self, var, epsilon_w, strategy):\n",
    "        # Helper function to apply epsilon_w on model variables.\n",
    "        if isinstance(\n",
    "            tf.distribute.get_strategy(),\n",
    "            (\n",
    "                tf.distribute.experimental.ParameterServerStrategy,\n",
    "                tf.distribute.experimental.CentralStorageStrategy,\n",
    "            ),\n",
    "        ):\n",
    "            # Under PSS and CSS, the AggregatingVariable has to be kept in sync.\n",
    "            def distribute_apply(strategy, var, epsilon_w):\n",
    "                strategy.extended.update(\n",
    "                    var,\n",
    "                    lambda x, y: x.assign_add(y),\n",
    "                    args=(epsilon_w,),\n",
    "                    group=False,\n",
    "                )\n",
    "\n",
    "            tf.__internal__.distribute.interim.maybe_merge_call(\n",
    "                distribute_apply, tf.distribute.get_strategy(), var, epsilon_w\n",
    "            )\n",
    "        else:\n",
    "            var.assign_add(epsilon_w)\n",
    "\n",
    "    def _gradients_order2_norm(self, gradients):\n",
    "        norm = tf.norm(\n",
    "            tf.stack([tf.norm(grad) for grad in gradients if grad is not None])\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5d831b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsam import GSAM, LinearScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4612dd6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mweight_decay)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "base_optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                 lr=args.learning_rate,\n",
    "                                 momentum=args.momentum,\n",
    "                                 weight_decay=args.weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d613fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow69)",
   "language": "python",
   "name": "tensorflow69"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
