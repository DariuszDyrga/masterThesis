{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7748e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "from keras.engine import data_adapter\n",
    "from keras.layers import deserialize as deserialize_layer\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "# isort: off\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "# @generic_utils.register_keras_serializable()\n",
    "# @keras_export(\"keras.models.experimental.SharpnessAwareMinimization\", v1=[])\n",
    "class SharpnessAwareMinimization(Model):\n",
    "    \"\"\"Sharpness aware minimization (SAM) training flow.\n",
    "\n",
    "    Sharpness-aware minimization (SAM) is a technique that improves the model\n",
    "    generalization and provides robustness to label noise. Mini-batch splitting\n",
    "    is proven to improve the SAM's performance, so users can control how mini\n",
    "    batches are split via setting the `num_batch_splits` argument.\n",
    "\n",
    "    Args:\n",
    "      model: `tf.keras.Model` instance. The inner model that does the\n",
    "        forward-backward pass.\n",
    "      rho: float, defaults to 0.05. The gradients scaling factor.\n",
    "      num_batch_splits: int, defaults to None. The number of mini batches to\n",
    "        split into from each data batch. If None, batches are not split into\n",
    "        sub-batches.\n",
    "      name: string, defaults to None. The name of the SAM model.\n",
    "\n",
    "    Reference:\n",
    "      [Pierre Foret et al., 2020](https://arxiv.org/abs/2010.01412)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, rho=0.05, num_batch_splits=None, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.model = model\n",
    "        self.rho = rho\n",
    "        self.num_batch_splits = num_batch_splits\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"The logic of one SAM training step.\n",
    "\n",
    "        Args:\n",
    "          data: A nested structure of `Tensor`s. It should be of structure\n",
    "            (x, y, sample_weight) or (x, y).\n",
    "\n",
    "        Returns:\n",
    "          A dict mapping metric names to running average values.\n",
    "        \"\"\"\n",
    "        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "        if self.num_batch_splits is not None:\n",
    "            x_split = tf.split(x, self.num_batch_splits)\n",
    "            y_split = tf.split(y, self.num_batch_splits)\n",
    "        else:\n",
    "            x_split = [x]\n",
    "            y_split = [y]\n",
    "\n",
    "        gradients_all_batches = []\n",
    "        pred_all_batches = []\n",
    "        for x_batch, y_batch in zip(x_split, y_split):\n",
    "            epsilon_w_cache = []\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = self.model(x_batch)\n",
    "                loss = self.compiled_loss(y_batch, pred)\n",
    "            pred_all_batches.append(pred)\n",
    "            trainable_variables = self.model.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "            gradients_order2_norm = self._gradients_order2_norm(gradients)\n",
    "            scale = self.rho / (gradients_order2_norm + 1e-12)\n",
    "\n",
    "            for gradient, variable in zip(gradients, trainable_variables):\n",
    "                epsilon_w = gradient * scale\n",
    "                self._distributed_apply_epsilon_w(\n",
    "                    variable, epsilon_w, tf.distribute.get_strategy()\n",
    "                )\n",
    "                epsilon_w_cache.append(epsilon_w)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = self(x_batch)\n",
    "                loss = self.compiled_loss(y_batch, pred)\n",
    "            gradients = tape.gradient(loss, trainable_variables)\n",
    "            if len(gradients_all_batches) == 0:\n",
    "                for gradient in gradients:\n",
    "                    gradients_all_batches.append([gradient])\n",
    "            else:\n",
    "                for gradient, gradient_all_batches in zip(\n",
    "                    gradients, gradients_all_batches\n",
    "                ):\n",
    "                    gradient_all_batches.append(gradient)\n",
    "            for variable, epsilon_w in zip(\n",
    "                trainable_variables, epsilon_w_cache\n",
    "            ):\n",
    "                # Restore the variable to its original value before\n",
    "                # `apply_gradients()`.\n",
    "                self._distributed_apply_epsilon_w(\n",
    "                    variable, -epsilon_w, tf.distribute.get_strategy()\n",
    "                )\n",
    "\n",
    "        gradients = []\n",
    "        for gradient_all_batches in gradients_all_batches:\n",
    "            gradients.append(tf.reduce_sum(gradient_all_batches, axis=0))\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        pred = tf.concat(pred_all_batches, axis=0)\n",
    "        self.compiled_metrics.update_state(y, pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of SAM.\n",
    "\n",
    "        SAM delegates the forward pass call to the wrapped model.\n",
    "\n",
    "        Args:\n",
    "          inputs: Tensor. The model inputs.\n",
    "\n",
    "        Returns:\n",
    "          A Tensor, the outputs of the wrapped model for given `inputs`.\n",
    "        \"\"\"\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"model\": generic_utils.serialize_keras_object(self.model),\n",
    "                \"rho\": self.rho,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, custom_objects=None):\n",
    "        # Avoid mutating the input dict.\n",
    "        config = copy.deepcopy(config)\n",
    "        model = deserialize_layer(\n",
    "            config.pop(\"model\"), custom_objects=custom_objects\n",
    "        )\n",
    "        config[\"model\"] = model\n",
    "        return super().from_config(config, custom_objects)\n",
    "\n",
    "    def _distributed_apply_epsilon_w(self, var, epsilon_w, strategy):\n",
    "        # Helper function to apply epsilon_w on model variables.\n",
    "        if isinstance(\n",
    "            tf.distribute.get_strategy(),\n",
    "            (\n",
    "                tf.distribute.experimental.ParameterServerStrategy,\n",
    "                tf.distribute.experimental.CentralStorageStrategy,\n",
    "            ),\n",
    "        ):\n",
    "            # Under PSS and CSS, the AggregatingVariable has to be kept in sync.\n",
    "            def distribute_apply(strategy, var, epsilon_w):\n",
    "                strategy.extended.update(\n",
    "                    var,\n",
    "                    lambda x, y: x.assign_add(y),\n",
    "                    args=(epsilon_w,),\n",
    "                    group=False,\n",
    "                )\n",
    "\n",
    "            tf.__internal__.distribute.interim.maybe_merge_call(\n",
    "                distribute_apply, tf.distribute.get_strategy(), var, epsilon_w\n",
    "            )\n",
    "        else:\n",
    "            var.assign_add(epsilon_w)\n",
    "\n",
    "    def _gradients_order2_norm(self, gradients):\n",
    "        norm = tf.norm(\n",
    "            tf.stack([tf.norm(grad) for grad in gradients if grad is not None])\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b83fe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddyrga/opt/miniconda3/envs/tensorflow69/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from random import shuffle as random_shuffle\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.applications import vgg16, inception_v3, resnet\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import os\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from vit_keras import vit\n",
    "import numpy as np\n",
    "import click\n",
    "from numpy.random import seed as np_seed\n",
    "from random import seed\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from load import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a747d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import click\n",
    "from numpy.random import seed as np_seed\n",
    "from random import seed\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e520c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xs_ys(ill_positive, negatives, index):\n",
    "    neg = negatives[index]\n",
    "    # np.random.shuffle(neg)\n",
    "    xs = np.concatenate((ill_positive[index], neg[:len(ill_positive[index])]))\n",
    "    ys = np.array(len(ill_positive[index]) * [1] + len(ill_positive[index]) * [0])\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84f23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepr(img_arr, img_size=75):\n",
    "    if img_arr.shape[-2] == img_size:\n",
    "        return preprocess_input(img_arr)\n",
    "    else:\n",
    "        return cv2.resize(preprocess_input(img_arr), dsize=(img_size, img_size), interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34408fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patient(path, img_size=75):\n",
    "    imgs = []\n",
    "    imgs_arr = np.load(path, allow_pickle=True)\n",
    "    for img_arr in imgs_arr:\n",
    "        imgs.append(prepr(img_arr, img_size))\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a29433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medtransfer_data_npy(path, i, img_size=75):\n",
    "    ill_path = os.path.join(path, 'ill/npy')\n",
    "    ill_positive = []\n",
    "    ill_negative = []\n",
    "\n",
    "    for file in sorted(os.listdir(ill_path)):\n",
    "        if 'pos' in file:\n",
    "            ill_positive.append(load_patient(os.path.join(ill_path, file), img_size))\n",
    "        else:\n",
    "            ill_negative.append(load_patient(os.path.join(ill_path, file), img_size))\n",
    "\n",
    "    healthy_path = os.path.join(path, 'healthy/npy')\n",
    "    healthy_negative = []\n",
    "    for file in sorted(os.listdir(healthy_path)):\n",
    "        healthy_negative.append(load_patient(os.path.join(healthy_path, file), img_size))\n",
    "\n",
    "    negatives = []\n",
    "    for ill, healthy in zip(ill_negative, healthy_negative):\n",
    "        negatives.append(np.concatenate((ill, healthy)))\n",
    "\n",
    "    for n in negatives:\n",
    "        print(n.shape)\n",
    "    for n in ill_positive:\n",
    "        print(n.shape)\n",
    "\n",
    "    test_xs, test_ys = extract_xs_ys(ill_positive, negatives, -i)\n",
    "    validation_xs, validation_ys = extract_xs_ys(ill_positive, negatives, -i-1)\n",
    "    train_positive = np.concatenate(ill_positive[:-i-2] + ill_positive[-i:])\n",
    "    train_negative = np.concatenate(negatives[:-i-2] + negatives[-i:])\n",
    "\n",
    "    print('POS')\n",
    "    print(len(train_positive))\n",
    "    print('NEG')\n",
    "    print(len(train_negative))\n",
    "    print()\n",
    "\n",
    "    np.random.seed(0)\n",
    "    random_shuffle(train_positive)\n",
    "    random_shuffle(train_negative)\n",
    "\n",
    "    len_train_pos = len(train_positive)\n",
    "    train_negative = train_negative[:len_train_pos]\n",
    "\n",
    "    print('NEG')\n",
    "    print(len(train_negative))\n",
    "    print()\n",
    "\n",
    "    train = [x for x in zip(np.concatenate((train_positive, train_negative)), [1] * len_train_pos + [0] * len_train_pos)]\n",
    "    random_shuffle(train)\n",
    "    train_xs, train_ys = list(zip(*train))\n",
    "\n",
    "    return np.array(train_xs), np.array(train_ys), validation_xs, validation_ys, test_xs, test_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4253a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(base_model_name, image_size, num_classes, activation='linear',\n",
    "                      include_top=True, pretrained=True, pretrained_top=True,\n",
    "                      weights=\"imagenet21k+imagenet2012\"):\n",
    "    if base_model_name == 'ViT_l16':\n",
    "        base_model = vit.vit_l16(image_size=image_size, classes=num_classes, activation=activation,\n",
    "                                 include_top=include_top, pretrained=pretrained,\n",
    "                                 pretrained_top=pretrained_top)\n",
    "    elif base_model_name == 'ViT_b16':\n",
    "        base_model = vit.vit_b16(image_size=image_size, classes=num_classes, activation=activation,\n",
    "                                 include_top=include_top, pretrained=pretrained,\n",
    "                                 pretrained_top=pretrained_top)\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect model name passed to transformer_model\")\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7653c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediatemodel(base_model, output_index_layer, unfreeze=False, unfreeze_from=0):\n",
    "    intermediate_layer_model = Model(inputs=base_model.input,\n",
    "                                     outputs=base_model.get_layer(index=output_index_layer).output)\n",
    "\n",
    "    i = 0\n",
    "    for layer in intermediate_layer_model.layers:\n",
    "        print(layer.name)\n",
    "        print(i)\n",
    "        i += 1\n",
    "        layer.trainable = False\n",
    "\n",
    "    if unfreeze:\n",
    "        for layer in intermediate_layer_model.layers[unfreeze_from:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    for layer in intermediate_layer_model.layers:\n",
    "        print(\"{}: {}\".format(layer, layer.trainable))\n",
    "\n",
    "    return intermediate_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f87ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionmodel(base_model, dense_sizes=[128], dense_activations=['relu'],\n",
    "                    dense_kernel_inits=['random_uniform'], dropout_p=[0.5],\n",
    "                    pooling='avg', classes=1,\n",
    "                    class_activation='sigmoid'):\n",
    "    x = base_model.output\n",
    "    if pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif pooling == 'max':\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    elif pooling == 'extract_token':  # Used with Vision Transformer Cut\n",
    "        x = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"Transformer/encoder_norm\")(x[0])\n",
    "        x = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(x)\n",
    "    elif pooling is None:\n",
    "        pass  # Pooling is not needed for Vision Transformer\n",
    "    else:\n",
    "        x = Flatten(name=\"flatten\")(x)\n",
    "\n",
    "    for i, (ds, da, dki, dop) in enumerate(zip(dense_sizes, dense_activations, dense_kernel_inits, dropout_p)):\n",
    "        x = Dense(ds, activation=da, kernel_initializer=dki)(x)\n",
    "        name = 'Dropout_Regularization_' + str(i)\n",
    "        x = Dropout(dop, name=name)(x)\n",
    "\n",
    "    predictions = Dense(classes, activation=class_activation, name='Output')(x)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4e3501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_dir, experiment_name, architecture_type, dataset, epochs, patience, seeds_nmb, folds_nmb, save_test_data,\n",
    "        sam_model):\n",
    "    batch_size = 100\n",
    "    dest_img_size = 64 \n",
    "    \n",
    "    seeds = [(1, 2, 3), (11, 22, 33), (111, 222, 333), (1111, 2222, 3333), (11111, 22222, 33333),\n",
    "             (71, 72, 73), (711, 722, 733), (7111, 7222, 7333), (71111, 72222, 73333), (711111, 722222, 733333)]\n",
    "\n",
    "    for seeds_set in seeds[:seeds_nmb]:\n",
    "\n",
    "        for i in range(folds_nmb):\n",
    "            print(f'TRAINING {i} model...')\n",
    "\n",
    "            np_seed(seeds_set[0])\n",
    "            seed(seeds_set[1])\n",
    "            tensorflow.random.set_seed(seeds_set[2])\n",
    "\n",
    "            print(\"WARTOSC I\")\n",
    "            print(i)\n",
    "            train_xs, train_ys, validation_xs, validation_ys, test_xs, test_ys = \\\n",
    "                load_medtransfer_data_npy(data_dir, i, dest_img_size)\n",
    "\n",
    "            if save_test_data:\n",
    "                np.save(f'{experiment_name}_{i}_test_xs.npy', test_xs)\n",
    "                np.save(f'{experiment_name}_{i}_test_ys.npy', test_ys)\n",
    "            if architecture_type == 'base_and_head':\n",
    "                base_model = transformer_model('ViT_b16', image_size=64, num_classes=1, pretrained=False,\n",
    "                                               pretrained_top=False, activation='sigmoid', include_top=False)\n",
    "            elif 'ViT' not in architecture_type:\n",
    "                base_model = basemodel('InceptionV3', input_shape=(75, 75, 3))\n",
    "            else:\n",
    "                base_model = transformer_model('ViT_b16', image_size=64, num_classes=1,\n",
    "                                             pretrained_top=False, activation='sigmoid', include_top=False)\n",
    "\n",
    "            if architecture_type == 'FNFE':\n",
    "                midmodel = intermediatemodel(base_model, len(base_model.layers) - 1)\n",
    "            elif architecture_type == 'FNFT':\n",
    "                midmodel = intermediatemodel(base_model, len(base_model.layers) - 1, unfreeze=True)\n",
    "            elif architecture_type == 'NCFE':\n",
    "                midmodel = intermediatemodel(base_model, 86)\n",
    "            elif architecture_type == 'NCFT':\n",
    "                midmodel = intermediatemodel(base_model, 86, unfreeze=True)\n",
    "            elif architecture_type == 'ViT-FE':\n",
    "                midmodel = intermediatemodel(base_model, len(base_model.layers) - 1)\n",
    "            elif architecture_type == 'ViT-FT':\n",
    "                midmodel = intermediatemodel(base_model, len(base_model.layers) - 1, unfreeze=True)\n",
    "            elif architecture_type == 'ViT-CFE':\n",
    "                midmodel = intermediatemodel(base_model, 10)\n",
    "            elif architecture_type == 'ViT-CFT':\n",
    "                midmodel = intermediatemodel(base_model, 10, unfreeze=True)\n",
    "            elif architecture_type == 'ViT-CFE-S':\n",
    "                midmodel = intermediatemodel(base_model, 5)\n",
    "            elif architecture_type == 'ViT-CFT-S':\n",
    "                midmodel = intermediatemodel(base_model, 5, unfreeze=True)\n",
    "            elif architecture_type == 'base_and_head':\n",
    "                midmodel = intermediatemodel(base_model, len(base_model.layers) - 1, unfreeze=True)\n",
    "            else:\n",
    "                raise ValueError('Architecture type not supported.')\n",
    "\n",
    "            if 'ViT-C' in architecture_type:  # Case for ViT Cut\n",
    "                headmodel = predictionmodel(midmodel, pooling='extract_token')\n",
    "            else:\n",
    "                headmodel = predictionmodel(midmodel, pooling=None)\n",
    "\n",
    "            model = Model(inputs=midmodel.input, outputs=headmodel)\n",
    "            if sam_model:\n",
    "                if not 'ViT' in architecture_type:\n",
    "                    raise ValueError('SAM is only supported with ViT architecture')\n",
    "                # SAM is built as a wrapper around base model\n",
    "                base_model_input_shape = model.input_shape\n",
    "                model = SharpnessAwareMinimization(model)\n",
    "                model.build(input_shape=base_model_input_shape)\n",
    "\n",
    "#             print_model(model)\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                mode='min',\n",
    "                monitor='val_loss',\n",
    "                factor=0.1,\n",
    "                min_lr=5e-7,\n",
    "                patience=10,\n",
    "                verbose=1)\n",
    "\n",
    "            es = EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=patience,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True,\n",
    "                verbose=1)\n",
    "\n",
    "            opt = SGD(learning_rate=0.001,\n",
    "                      momentum=0.9,\n",
    "                      name=\"SGD\")\n",
    "\n",
    "            model.compile(opt,\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['acc'])\n",
    "\n",
    "            model.fit(x=train_xs,\n",
    "                      y=train_ys,\n",
    "                      validation_data=(validation_xs, validation_ys),\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      callbacks=[reduce_lr, es])\n",
    "\n",
    "            stopped_epoch = es.stopped_epoch\n",
    "            if not sam_model:\n",
    "                model.save(f'model-{experiment_name}_{i}_epoch{stopped_epoch}-{seeds_set[0]}.h5')\n",
    "            else:\n",
    "                # Custom models are not serializable\n",
    "                model.save(f'model-{experiment_name}_{i}_epoch{stopped_epoch}-{seeds_set[0]}', save_format='tf')\n",
    "            model_return = model\n",
    "            preds = model.predict(test_xs, verbose=1)\n",
    "            print(preds)\n",
    "            print()\n",
    "            print([round(float(x), 3) for x in preds])\n",
    "            preds = preds > 0.5\n",
    "            print()\n",
    "            print([int(x) for x in preds])\n",
    "\n",
    "            print()\n",
    "            results = [accuracy_score(preds, test_ys),\n",
    "                       f1_score(preds, test_ys),\n",
    "                       precision_score(preds, test_ys),\n",
    "                       recall_score(preds, test_ys)]\n",
    "            print(f'ACC: {results[0]}')\n",
    "            print(f'F1: {results[1]}')\n",
    "            print(f'PREC: {results[2]}')\n",
    "            print(f'REC: {results[3]}')\n",
    "            print()\n",
    "            print('&' * 50)\n",
    "            print()\n",
    "\n",
    "            with open(f'results-{experiment_name}.txt', 'a') as f:\n",
    "                f.write(str(results))\n",
    "\n",
    "            i += 1\n",
    "            break\n",
    "        break\n",
    "    return model_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dfe2aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING 0 model...\n",
      "WARTOSC I\n",
      "0\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(30, 64, 64, 3)\n",
      "(70, 64, 64, 3)\n",
      "(20, 64, 64, 3)\n",
      "(70, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(150, 64, 64, 3)\n",
      "(50, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "POS\n",
      "1330\n",
      "NEG\n",
      "2800\n",
      "\n",
      "NEG\n",
      "1330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddyrga/opt/miniconda3/envs/tensorflow69/lib/python3.8/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 4, 4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "0\n",
      "embedding\n",
      "1\n",
      "reshape\n",
      "2\n",
      "class_token\n",
      "3\n",
      "Transformer/posembed_input\n",
      "4\n",
      "Transformer/encoderblock_0\n",
      "5\n",
      "Transformer/encoderblock_1\n",
      "6\n",
      "Transformer/encoderblock_2\n",
      "7\n",
      "Transformer/encoderblock_3\n",
      "8\n",
      "Transformer/encoderblock_4\n",
      "9\n",
      "Transformer/encoderblock_5\n",
      "10\n",
      "<keras.engine.input_layer.InputLayer object at 0x105ff6cd0>: False\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x296cf0910>: False\n",
      "<keras.layers.reshaping.reshape.Reshape object at 0x296cf0fd0>: False\n",
      "<vit_keras.layers.ClassToken object at 0x296d1f7f0>: False\n",
      "<vit_keras.layers.AddPositionEmbs object at 0x296cb6ee0>: False\n",
      "<vit_keras.layers.TransformerBlock object at 0x296facdc0>: False\n",
      "<vit_keras.layers.TransformerBlock object at 0x2966b05b0>: False\n",
      "<vit_keras.layers.TransformerBlock object at 0x29f35ffd0>: False\n",
      "<vit_keras.layers.TransformerBlock object at 0x29f520df0>: False\n",
      "<vit_keras.layers.TransformerBlock object at 0x29f539100>: False\n",
      "<vit_keras.layers.TransformerBlock object at 0x29f748f40>: False\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 13:39:59.910732: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 15s 501ms/step - loss: 0.6918 - acc: 0.5880 - val_loss: 0.5663 - val_acc: 0.7550 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 13s 498ms/step - loss: 0.5635 - acc: 0.7214 - val_loss: 0.4940 - val_acc: 0.8575 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 13s 491ms/step - loss: 0.5070 - acc: 0.7774 - val_loss: 0.4802 - val_acc: 0.7675 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 13s 486ms/step - loss: 0.4727 - acc: 0.7910 - val_loss: 0.4244 - val_acc: 0.8675 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 13s 483ms/step - loss: 0.4477 - acc: 0.8102 - val_loss: 0.4246 - val_acc: 0.8075 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 13s 485ms/step - loss: 0.4270 - acc: 0.8109 - val_loss: 0.3893 - val_acc: 0.8725 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 13s 488ms/step - loss: 0.4238 - acc: 0.8098 - val_loss: 0.3989 - val_acc: 0.8325 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 13s 494ms/step - loss: 0.3979 - acc: 0.8361 - val_loss: 0.3607 - val_acc: 0.8875 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 13s 492ms/step - loss: 0.3875 - acc: 0.8361 - val_loss: 0.3518 - val_acc: 0.8900 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 13s 497ms/step - loss: 0.3772 - acc: 0.8444 - val_loss: 0.3587 - val_acc: 0.8550 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 44s 2s/step - loss: 0.3718 - acc: 0.8508 - val_loss: 0.3321 - val_acc: 0.8950 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 13s 500ms/step - loss: 0.3644 - acc: 0.8523 - val_loss: 0.3337 - val_acc: 0.9025 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 13s 501ms/step - loss: 0.3608 - acc: 0.8515 - val_loss: 0.3593 - val_acc: 0.8375 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 13s 501ms/step - loss: 0.3564 - acc: 0.8571 - val_loss: 0.3234 - val_acc: 0.9025 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 14s 512ms/step - loss: 0.3383 - acc: 0.8680 - val_loss: 0.3086 - val_acc: 0.9075 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 13s 492ms/step - loss: 0.3359 - acc: 0.8662 - val_loss: 0.3099 - val_acc: 0.9025 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 14s 507ms/step - loss: 0.3316 - acc: 0.8680 - val_loss: 0.3102 - val_acc: 0.9050 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 13s 488ms/step - loss: 0.3297 - acc: 0.8594 - val_loss: 0.3098 - val_acc: 0.9025 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 13s 496ms/step - loss: 0.3197 - acc: 0.8733 - val_loss: 0.2859 - val_acc: 0.8950 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 13s 487ms/step - loss: 0.3238 - acc: 0.8692 - val_loss: 0.3026 - val_acc: 0.9025 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 13s 487ms/step - loss: 0.3096 - acc: 0.8744 - val_loss: 0.2912 - val_acc: 0.9050 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 13s 488ms/step - loss: 0.3199 - acc: 0.8684 - val_loss: 0.2790 - val_acc: 0.9025 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 13s 487ms/step - loss: 0.3227 - acc: 0.8662 - val_loss: 0.2929 - val_acc: 0.9050 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 13s 491ms/step - loss: 0.3110 - acc: 0.8733 - val_loss: 0.2950 - val_acc: 0.9000 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 13s 494ms/step - loss: 0.3165 - acc: 0.8714 - val_loss: 0.2690 - val_acc: 0.9075 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 13s 502ms/step - loss: 0.3043 - acc: 0.8797 - val_loss: 0.2746 - val_acc: 0.9125 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 102s 4s/step - loss: 0.3072 - acc: 0.8763 - val_loss: 0.2666 - val_acc: 0.9200 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 13s 492ms/step - loss: 0.3037 - acc: 0.8778 - val_loss: 0.2712 - val_acc: 0.9050 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 14s 503ms/step - loss: 0.2976 - acc: 0.8835 - val_loss: 0.2633 - val_acc: 0.9250 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 14s 514ms/step - loss: 0.3046 - acc: 0.8733 - val_loss: 0.2780 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 14s 504ms/step - loss: 0.2853 - acc: 0.8868 - val_loss: 0.2628 - val_acc: 0.9125 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 14s 534ms/step - loss: 0.2898 - acc: 0.8846 - val_loss: 0.2652 - val_acc: 0.9125 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 14s 537ms/step - loss: 0.2754 - acc: 0.8951 - val_loss: 0.2549 - val_acc: 0.9225 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 14s 511ms/step - loss: 0.2801 - acc: 0.8812 - val_loss: 0.2637 - val_acc: 0.9200 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 13s 502ms/step - loss: 0.2845 - acc: 0.8883 - val_loss: 0.2638 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 14s 526ms/step - loss: 0.2797 - acc: 0.8898 - val_loss: 0.2530 - val_acc: 0.9275 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 15s 560ms/step - loss: 0.2779 - acc: 0.8921 - val_loss: 0.2527 - val_acc: 0.9275 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 15s 557ms/step - loss: 0.2665 - acc: 0.9015 - val_loss: 0.2426 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 14s 533ms/step - loss: 0.2717 - acc: 0.8883 - val_loss: 0.2420 - val_acc: 0.9125 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 15s 558ms/step - loss: 0.2747 - acc: 0.8936 - val_loss: 0.2509 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 16s 572ms/step - loss: 0.2801 - acc: 0.8925 - val_loss: 0.2515 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 14s 522ms/step - loss: 0.2652 - acc: 0.8914 - val_loss: 0.2424 - val_acc: 0.9300 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 14s 513ms/step - loss: 0.2664 - acc: 0.8974 - val_loss: 0.2671 - val_acc: 0.9150 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 14s 512ms/step - loss: 0.2644 - acc: 0.8981 - val_loss: 0.2378 - val_acc: 0.9250 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 14s 504ms/step - loss: 0.2626 - acc: 0.9000 - val_loss: 0.2413 - val_acc: 0.9250 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 13s 497ms/step - loss: 0.2684 - acc: 0.8925 - val_loss: 0.2476 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 14s 508ms/step - loss: 0.2626 - acc: 0.8996 - val_loss: 0.2423 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 14s 508ms/step - loss: 0.2609 - acc: 0.8962 - val_loss: 0.2470 - val_acc: 0.9175 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 14s 506ms/step - loss: 0.2569 - acc: 0.8966 - val_loss: 0.2322 - val_acc: 0.9350 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 14s 526ms/step - loss: 0.2596 - acc: 0.8985 - val_loss: 0.2341 - val_acc: 0.9300 - lr: 0.0010\n",
      "2/2 [==============================] - 1s 167ms/step\n",
      "[[0.800303  ]\n",
      " [0.7536454 ]\n",
      " [0.8214213 ]\n",
      " [0.8429287 ]\n",
      " [0.70780015]\n",
      " [0.7370222 ]\n",
      " [0.79190314]\n",
      " [0.67955494]\n",
      " [0.7946886 ]\n",
      " [0.9480909 ]\n",
      " [0.8034485 ]\n",
      " [0.74563915]\n",
      " [0.8702043 ]\n",
      " [0.85041964]\n",
      " [0.89870304]\n",
      " [0.9430938 ]\n",
      " [0.0067651 ]\n",
      " [0.6953901 ]\n",
      " [0.5997895 ]\n",
      " [0.9018844 ]\n",
      " [0.47794962]\n",
      " [0.7071898 ]\n",
      " [0.94880915]\n",
      " [0.80613184]\n",
      " [0.95468587]\n",
      " [0.9233347 ]\n",
      " [0.57092804]\n",
      " [0.75927544]\n",
      " [0.929546  ]\n",
      " [0.884831  ]\n",
      " [0.0039335 ]\n",
      " [0.8523716 ]\n",
      " [0.01409059]\n",
      " [0.19839667]\n",
      " [0.22869195]\n",
      " [0.03247596]\n",
      " [0.61450136]\n",
      " [0.7790074 ]\n",
      " [0.03550191]\n",
      " [0.6658801 ]\n",
      " [0.15820508]\n",
      " [0.06460084]\n",
      " [0.19818896]\n",
      " [0.02121078]\n",
      " [0.19133373]\n",
      " [0.49342683]\n",
      " [0.26152295]\n",
      " [0.49361283]\n",
      " [0.94974643]\n",
      " [0.1556909 ]\n",
      " [0.06214631]\n",
      " [0.09574454]\n",
      " [0.01231022]\n",
      " [0.03323504]\n",
      " [0.392872  ]\n",
      " [0.60046434]\n",
      " [0.87871045]\n",
      " [0.25835922]\n",
      " [0.21110475]\n",
      " [0.76563466]]\n",
      "\n",
      "[0.8, 0.754, 0.821, 0.843, 0.708, 0.737, 0.792, 0.68, 0.795, 0.948, 0.803, 0.746, 0.87, 0.85, 0.899, 0.943, 0.007, 0.695, 0.6, 0.902, 0.478, 0.707, 0.949, 0.806, 0.955, 0.923, 0.571, 0.759, 0.93, 0.885, 0.004, 0.852, 0.014, 0.198, 0.229, 0.032, 0.615, 0.779, 0.036, 0.666, 0.158, 0.065, 0.198, 0.021, 0.191, 0.493, 0.262, 0.494, 0.95, 0.156, 0.062, 0.096, 0.012, 0.033, 0.393, 0.6, 0.879, 0.258, 0.211, 0.766]\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "\n",
      "ACC: 0.8333333333333334\n",
      "F1: 0.8484848484848485\n",
      "PREC: 0.9333333333333333\n",
      "REC: 0.7777777777777778\n",
      "\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod_final = run(data_dir='data/PATIENTS-DATA-ratio-0.5/', architecture_type='ViT-CFE', dataset='medtransfer', epochs=50, patience=5,\n",
    "   sam_model=False,seeds_nmb=7, folds_nmb=7, save_test_data=False, experiment_name=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "976c53f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(30, 64, 64, 3)\n",
      "(70, 64, 64, 3)\n",
      "(20, 64, 64, 3)\n",
      "(70, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "(150, 64, 64, 3)\n",
      "(50, 64, 64, 3)\n",
      "(200, 64, 64, 3)\n",
      "POS\n",
      "440\n",
      "NEG\n",
      "1200\n",
      "\n",
      "NEG\n",
      "440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir='data/PATIENTS-DATA-ratio-0.5/'\n",
    "i=2\n",
    "dest_img_size = 64\n",
    "train_xs, train_ys, validation_xs, validation_ys, test_xs, test_ys = \\\n",
    "                load_medtransfer_data_npy(data_dir, i, dest_img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33147e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.15111233,  0.15111233,  0.15111233],\n",
       "         [ 0.15831302,  0.15831302,  0.15831302],\n",
       "         [ 0.16976322,  0.16976322,  0.16976322],\n",
       "         ...,\n",
       "         [ 0.19693524,  0.19693524,  0.19693524],\n",
       "         [ 0.15774612,  0.15774612,  0.15774612],\n",
       "         [ 0.13360445,  0.13360445,  0.13360445]],\n",
       "\n",
       "        [[ 0.14985561,  0.14985561,  0.14985561],\n",
       "         [ 0.15562102,  0.15562102,  0.15562102],\n",
       "         [ 0.16477868,  0.16477868,  0.16477868],\n",
       "         ...,\n",
       "         [ 0.20801863,  0.20801863,  0.20801863],\n",
       "         [ 0.16717485,  0.16717485,  0.16717485],\n",
       "         [ 0.14200482,  0.14200482,  0.14200482]],\n",
       "\n",
       "        [[ 0.14806738,  0.14806738,  0.14806738],\n",
       "         [ 0.15150797,  0.15150797,  0.15150797],\n",
       "         [ 0.15695661,  0.15695661,  0.15695661],\n",
       "         ...,\n",
       "         [ 0.22589499,  0.22589499,  0.22589499],\n",
       "         [ 0.18217993,  0.18217993,  0.18217993],\n",
       "         [ 0.15522598,  0.15522598,  0.15522598]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.02997486,  0.02997486,  0.02997486],\n",
       "         [ 0.02295545,  0.02295545,  0.02295545],\n",
       "         [ 0.01185711,  0.01185711,  0.01185711],\n",
       "         ...,\n",
       "         [ 0.2881341 ,  0.2881341 ,  0.2881341 ],\n",
       "         [ 0.2721563 ,  0.2721563 ,  0.2721563 ],\n",
       "         [ 0.26268995,  0.26268995,  0.26268995]],\n",
       "\n",
       "        [[ 0.07296015,  0.07296015,  0.07296015],\n",
       "         [ 0.0714801 ,  0.0714801 ,  0.0714801 ],\n",
       "         [ 0.06938529,  0.06938529,  0.06938529],\n",
       "         ...,\n",
       "         [ 0.28120863,  0.28120863,  0.28120863],\n",
       "         [ 0.27272448,  0.27272448,  0.27272448],\n",
       "         [ 0.2677267 ,  0.2677267 ,  0.2677267 ]],\n",
       "\n",
       "        [[ 0.09977363,  0.09977363,  0.09977363],\n",
       "         [ 0.1016413 ,  0.1016413 ,  0.1016413 ],\n",
       "         [ 0.1049889 ,  0.1049889 ,  0.1049889 ],\n",
       "         ...,\n",
       "         [ 0.2770422 ,  0.2770422 ,  0.2770422 ],\n",
       "         [ 0.2731235 ,  0.2731235 ,  0.2731235 ],\n",
       "         [ 0.27085   ,  0.27085   ,  0.27085   ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.293116  ,  0.293116  ,  0.293116  ],\n",
       "         [ 0.30355957,  0.30355957,  0.30355957],\n",
       "         [ 0.3205764 ,  0.3205764 ,  0.3205764 ],\n",
       "         ...,\n",
       "         [ 0.33306843,  0.33306843,  0.33306843],\n",
       "         [ 0.33497703,  0.33497703,  0.33497703],\n",
       "         [ 0.33607674,  0.33607674,  0.33607674]],\n",
       "\n",
       "        [[ 0.28607038,  0.28607038,  0.28607038],\n",
       "         [ 0.2948568 ,  0.2948568 ,  0.2948568 ],\n",
       "         [ 0.30910504,  0.30910504,  0.30910504],\n",
       "         ...,\n",
       "         [ 0.33153966,  0.33153966,  0.33153966],\n",
       "         [ 0.32939985,  0.32939985,  0.32939985],\n",
       "         [ 0.32804987,  0.32804987,  0.32804987]],\n",
       "\n",
       "        [[ 0.27454263,  0.27454263,  0.27454263],\n",
       "         [ 0.28065738,  0.28065738,  0.28065738],\n",
       "         [ 0.29043832,  0.29043832,  0.29043832],\n",
       "         ...,\n",
       "         [ 0.3291069 ,  0.3291069 ,  0.3291069 ],\n",
       "         [ 0.3203781 ,  0.3203781 ,  0.3203781 ],\n",
       "         [ 0.31504434,  0.31504434,  0.31504434]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.23940781,  0.23940781,  0.23940781],\n",
       "         [ 0.24075203,  0.24075203,  0.24075203],\n",
       "         [ 0.24263261,  0.24263261,  0.24263261],\n",
       "         ...,\n",
       "         [ 0.11712053,  0.11712053,  0.11712053],\n",
       "         [ 0.11241389,  0.11241389,  0.11241389],\n",
       "         [ 0.10961538,  0.10961538,  0.10961538]],\n",
       "\n",
       "        [[ 0.25597227,  0.25597227,  0.25597227],\n",
       "         [ 0.256181  ,  0.256181  ,  0.256181  ],\n",
       "         [ 0.25635785,  0.25635785,  0.25635785],\n",
       "         ...,\n",
       "         [ 0.11098543,  0.11098543,  0.11098543],\n",
       "         [ 0.11217645,  0.11217645,  0.11217645],\n",
       "         [ 0.11295865,  0.11295865,  0.11295865]],\n",
       "\n",
       "        [[ 0.2661412 ,  0.2661412 ,  0.2661412 ],\n",
       "         [ 0.26567808,  0.26567808,  0.26567808],\n",
       "         [ 0.26484868,  0.26484868,  0.26484868],\n",
       "         ...,\n",
       "         [ 0.10720699,  0.10720699,  0.10720699],\n",
       "         [ 0.11184949,  0.11184949,  0.11184949],\n",
       "         [ 0.11472724,  0.11472724,  0.11472724]]],\n",
       "\n",
       "\n",
       "       [[[ 0.43760857,  0.43760857,  0.43760857],\n",
       "         [ 0.43500763,  0.43500763,  0.43500763],\n",
       "         [ 0.43101573,  0.43101573,  0.43101573],\n",
       "         ...,\n",
       "         [ 0.4831189 ,  0.4831189 ,  0.4831189 ],\n",
       "         [ 0.502788  ,  0.502788  ,  0.502788  ],\n",
       "         [ 0.5153799 ,  0.5153799 ,  0.5153799 ]],\n",
       "\n",
       "        [[ 0.45844516,  0.45844516,  0.45844516],\n",
       "         [ 0.4584867 ,  0.4584867 ,  0.4584867 ],\n",
       "         [ 0.45886412,  0.45886412,  0.45886412],\n",
       "         ...,\n",
       "         [ 0.49347308,  0.49347308,  0.49347308],\n",
       "         [ 0.5129914 ,  0.5129914 ,  0.5129914 ],\n",
       "         [ 0.5254918 ,  0.5254918 ,  0.5254918 ]],\n",
       "\n",
       "        [[ 0.49305612,  0.49305612,  0.49305612],\n",
       "         [ 0.4974671 ,  0.4974671 ,  0.4974671 ],\n",
       "         [ 0.5050754 ,  0.5050754 ,  0.5050754 ],\n",
       "         ...,\n",
       "         [ 0.51050496,  0.51050496,  0.51050496],\n",
       "         [ 0.52972174,  0.52972174,  0.52972174],\n",
       "         [ 0.542039  ,  0.542039  ,  0.542039  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.32990092,  0.32990092,  0.32990092],\n",
       "         [ 0.32976654,  0.32976654,  0.32976654],\n",
       "         [ 0.3298029 ,  0.3298029 ,  0.3298029 ],\n",
       "         ...,\n",
       "         [ 0.13504045,  0.13504045,  0.13504045],\n",
       "         [ 0.14957783,  0.14957783,  0.14957783],\n",
       "         [ 0.15858781,  0.15858781,  0.15858781]],\n",
       "\n",
       "        [[ 0.3275378 ,  0.3275378 ,  0.3275378 ],\n",
       "         [ 0.3262356 ,  0.3262356 ,  0.3262356 ],\n",
       "         [ 0.32437435,  0.32437435,  0.32437435],\n",
       "         ...,\n",
       "         [ 0.17032404,  0.17032404,  0.17032404],\n",
       "         [ 0.1862918 ,  0.1862918 ,  0.1862918 ],\n",
       "         [ 0.1960816 ,  0.1960816 ,  0.1960816 ]],\n",
       "\n",
       "        [[ 0.3264047 ,  0.3264047 ,  0.3264047 ],\n",
       "         [ 0.3243822 ,  0.3243822 ,  0.3243822 ],\n",
       "         [ 0.3213531 ,  0.3213531 ,  0.3213531 ],\n",
       "         ...,\n",
       "         [ 0.19241601,  0.19241601,  0.19241601],\n",
       "         [ 0.2091959 ,  0.2091959 ,  0.2091959 ],\n",
       "         [ 0.21942405,  0.21942405,  0.21942405]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-0.39590392, -0.39590392, -0.39590392],\n",
       "         [-0.39648232, -0.39648232, -0.39648232],\n",
       "         [-0.39744523, -0.39744523, -0.39744523],\n",
       "         ...,\n",
       "         [-0.10994406, -0.10994406, -0.10994406],\n",
       "         [-0.10836504, -0.10836504, -0.10836504],\n",
       "         [-0.107624  , -0.107624  , -0.107624  ]],\n",
       "\n",
       "        [[-0.39675808, -0.39675808, -0.39675808],\n",
       "         [-0.39543372, -0.39543372, -0.39543372],\n",
       "         [-0.3932338 , -0.3932338 , -0.3932338 ],\n",
       "         ...,\n",
       "         [-0.10970327, -0.10970327, -0.10970327],\n",
       "         [-0.1058255 , -0.1058255 , -0.1058255 ],\n",
       "         [-0.10376634, -0.10376634, -0.10376634]],\n",
       "\n",
       "        [[-0.39827245, -0.39827245, -0.39827245],\n",
       "         [-0.3937853 , -0.3937853 , -0.3937853 ],\n",
       "         [-0.38632965, -0.38632965, -0.38632965],\n",
       "         ...,\n",
       "         [-0.11005823, -0.11005823, -0.11005823],\n",
       "         [-0.10227948, -0.10227948, -0.10227948],\n",
       "         [-0.0979862 , -0.0979862 , -0.0979862 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.43270656, -0.43270656, -0.43270656],\n",
       "         [-0.43209583, -0.43209583, -0.43209583],\n",
       "         [-0.43116525, -0.43116525, -0.43116525],\n",
       "         ...,\n",
       "         [-0.33840573, -0.33840573, -0.33840573],\n",
       "         [-0.3513078 , -0.3513078 , -0.3513078 ],\n",
       "         [-0.3588791 , -0.3588791 , -0.3588791 ]],\n",
       "\n",
       "        [[-0.41801   , -0.41801   , -0.41801   ],\n",
       "         [-0.41779783, -0.41779783, -0.41779783],\n",
       "         [-0.41747037, -0.41747037, -0.41747037],\n",
       "         ...,\n",
       "         [-0.33560526, -0.33560526, -0.33560526],\n",
       "         [-0.3504739 , -0.3504739 , -0.3504739 ],\n",
       "         [-0.35921514, -0.35921514, -0.35921514]],\n",
       "\n",
       "        [[-0.40928307, -0.40928307, -0.40928307],\n",
       "         [-0.40925398, -0.40925398, -0.40925398],\n",
       "         [-0.4091958 , -0.4091958 , -0.4091958 ],\n",
       "         ...,\n",
       "         [-0.33380112, -0.33380112, -0.33380112],\n",
       "         [-0.34993666, -0.34993666, -0.34993666],\n",
       "         [-0.3594316 , -0.3594316 , -0.3594316 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.5590427 ,  0.5590427 ,  0.5590427 ],\n",
       "         [ 0.5616458 ,  0.5616458 ,  0.5616458 ],\n",
       "         [ 0.5658359 ,  0.5658359 ,  0.5658359 ],\n",
       "         ...,\n",
       "         [ 0.19853304,  0.19853304,  0.19853304],\n",
       "         [ 0.1995369 ,  0.1995369 ,  0.1995369 ],\n",
       "         [ 0.2000873 ,  0.2000873 ,  0.2000873 ]],\n",
       "\n",
       "        [[ 0.5616167 ,  0.5616167 ,  0.5616167 ],\n",
       "         [ 0.5634467 ,  0.5634467 ,  0.5634467 ],\n",
       "         [ 0.56636345,  0.56636345,  0.56636345],\n",
       "         ...,\n",
       "         [ 0.2005472 ,  0.2005472 ,  0.2005472 ],\n",
       "         [ 0.20012692,  0.20012692,  0.20012692],\n",
       "         [ 0.19981264,  0.19981264,  0.19981264]],\n",
       "\n",
       "        [[ 0.5657487 ,  0.5657487 ,  0.5657487 ],\n",
       "         [ 0.5663376 ,  0.5663376 ,  0.5663376 ],\n",
       "         [ 0.5672102 ,  0.5672102 ,  0.5672102 ],\n",
       "         ...,\n",
       "         [ 0.20386465,  0.20386465,  0.20386465],\n",
       "         [ 0.2010987 ,  0.2010987 ,  0.2010987 ],\n",
       "         [ 0.19936025,  0.19936025,  0.19936025]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.07700668,  0.07700668,  0.07700668],\n",
       "         [ 0.06655508,  0.06655508,  0.06655508],\n",
       "         [ 0.04971967,  0.04971967,  0.04971967],\n",
       "         ...,\n",
       "         [ 0.12353843,  0.12353843,  0.12353843],\n",
       "         [ 0.14314221,  0.14314221,  0.14314221],\n",
       "         [ 0.1551306 ,  0.1551306 ,  0.1551306 ]],\n",
       "\n",
       "        [[ 0.09899825,  0.09899825,  0.09899825],\n",
       "         [ 0.09302159,  0.09302159,  0.09302159],\n",
       "         [ 0.08356353,  0.08356353,  0.08356353],\n",
       "         ...,\n",
       "         [ 0.11524604,  0.11524604,  0.11524604],\n",
       "         [ 0.12974057,  0.12974057,  0.12974057],\n",
       "         [ 0.13860705,  0.13860705,  0.13860705]],\n",
       "\n",
       "        [[ 0.11306553,  0.11306553,  0.11306553],\n",
       "         [ 0.1097841 ,  0.1097841 ,  0.1097841 ],\n",
       "         [ 0.10476869,  0.10476869,  0.10476869],\n",
       "         ...,\n",
       "         [ 0.109924  ,  0.109924  ,  0.109924  ],\n",
       "         [ 0.12139358,  0.12139358,  0.12139358],\n",
       "         [ 0.12841012,  0.12841012,  0.12841012]]],\n",
       "\n",
       "\n",
       "       [[[-0.2558164 , -0.2558164 , -0.2558164 ],\n",
       "         [-0.2343392 , -0.2343392 , -0.2343392 ],\n",
       "         [-0.1997538 , -0.1997538 , -0.1997538 ],\n",
       "         ...,\n",
       "         [-0.05863611, -0.05863611, -0.05863611],\n",
       "         [-0.05763226, -0.05763226, -0.05763226],\n",
       "         [-0.05708187, -0.05708187, -0.05708187]],\n",
       "\n",
       "        [[-0.27575144, -0.27575144, -0.27575144],\n",
       "         [-0.25442588, -0.25442588, -0.25442588],\n",
       "         [-0.22006476, -0.22006476, -0.22006476],\n",
       "         ...,\n",
       "         [-0.06091153, -0.06091153, -0.06091153],\n",
       "         [-0.06120043, -0.06120043, -0.06120043],\n",
       "         [-0.06143284, -0.06143284, -0.06143284]],\n",
       "\n",
       "        [[-0.3083655 , -0.3083655 , -0.3083655 ],\n",
       "         [-0.28707036, -0.28707036, -0.28707036],\n",
       "         [-0.25271976, -0.25271976, -0.25271976],\n",
       "         ...,\n",
       "         [-0.064335  , -0.064335  , -0.064335  ],\n",
       "         [-0.06670675, -0.06670675, -0.06670675],\n",
       "         [-0.06819961, -0.06819961, -0.06819961]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.31275082, -0.31275082, -0.31275082],\n",
       "         [-0.30139166, -0.30139166, -0.30139166],\n",
       "         [-0.28286695, -0.28286695, -0.28286695],\n",
       "         ...,\n",
       "         [-0.25878033, -0.25878033, -0.25878033],\n",
       "         [-0.26754904, -0.26754904, -0.26754904],\n",
       "         [-0.27342337, -0.27342337, -0.27342337]],\n",
       "\n",
       "        [[-0.31296626, -0.31296626, -0.31296626],\n",
       "         [-0.30268443, -0.30268443, -0.30268443],\n",
       "         [-0.28594097, -0.28594097, -0.28594097],\n",
       "         ...,\n",
       "         [-0.2452968 , -0.2452968 , -0.2452968 ],\n",
       "         [-0.25947636, -0.25947636, -0.25947636],\n",
       "         [-0.26862335, -0.26862335, -0.26862335]],\n",
       "\n",
       "        [[-0.31302547, -0.31302547, -0.31302547],\n",
       "         [-0.30343494, -0.30343494, -0.30343494],\n",
       "         [-0.28783342, -0.28783342, -0.28783342],\n",
       "         ...,\n",
       "         [-0.23692505, -0.23692505, -0.23692505],\n",
       "         [-0.25431266, -0.25431266, -0.25431266],\n",
       "         [-0.26540118, -0.26540118, -0.26540118]]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535da848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 148ms/step\n",
      "[[7.07375705e-01]\n",
      " [1.04917116e-01]\n",
      " [5.06313257e-02]\n",
      " [9.48035121e-01]\n",
      " [7.83346966e-02]\n",
      " [2.69146055e-01]\n",
      " [9.82125401e-01]\n",
      " [2.99009502e-01]\n",
      " [9.34867024e-01]\n",
      " [9.27791357e-01]\n",
      " [9.44488227e-01]\n",
      " [6.14348173e-01]\n",
      " [6.26492679e-01]\n",
      " [9.13900971e-01]\n",
      " [3.74164373e-01]\n",
      " [9.10637796e-01]\n",
      " [9.85055208e-01]\n",
      " [9.69582736e-01]\n",
      " [8.73328865e-01]\n",
      " [9.25235510e-01]\n",
      " [8.77177000e-01]\n",
      " [2.44862542e-01]\n",
      " [9.58910584e-01]\n",
      " [7.49751151e-01]\n",
      " [3.57887238e-01]\n",
      " [1.80064961e-01]\n",
      " [7.69063354e-01]\n",
      " [9.18373942e-01]\n",
      " [8.24429631e-01]\n",
      " [8.55616808e-01]\n",
      " [9.52919543e-01]\n",
      " [5.39028198e-02]\n",
      " [3.19467604e-01]\n",
      " [7.49625087e-01]\n",
      " [1.19039677e-02]\n",
      " [2.49359682e-02]\n",
      " [2.00683728e-01]\n",
      " [9.33390439e-01]\n",
      " [4.91070271e-01]\n",
      " [3.56930435e-01]\n",
      " [3.60077828e-01]\n",
      " [7.68498003e-01]\n",
      " [2.44862542e-01]\n",
      " [7.79147923e-01]\n",
      " [3.43460701e-02]\n",
      " [8.05638060e-02]\n",
      " [4.88558233e-01]\n",
      " [6.81278467e-01]\n",
      " [7.02851236e-01]\n",
      " [9.40778434e-01]\n",
      " [5.11177294e-02]\n",
      " [3.27543635e-03]\n",
      " [5.54339774e-02]\n",
      " [3.63340601e-02]\n",
      " [1.81094818e-02]\n",
      " [2.36963391e-01]\n",
      " [2.17687525e-03]\n",
      " [5.21792471e-03]\n",
      " [1.43541943e-03]\n",
      " [2.05195602e-03]\n",
      " [2.58361213e-02]\n",
      " [3.32896179e-03]\n",
      " [6.76371008e-02]\n",
      " [4.31597582e-04]\n",
      " [2.02458948e-02]\n",
      " [8.87138784e-01]\n",
      " [1.50056452e-01]\n",
      " [6.05488988e-03]\n",
      " [1.79279111e-02]\n",
      " [4.07914191e-01]\n",
      " [3.12252402e-01]\n",
      " [3.67446453e-04]\n",
      " [7.85661116e-02]\n",
      " [8.86538718e-03]\n",
      " [4.43022652e-03]\n",
      " [1.41032329e-02]\n",
      " [3.66913644e-03]\n",
      " [5.62385377e-03]\n",
      " [1.18767796e-02]\n",
      " [1.06643718e-02]\n",
      " [1.18931577e-01]\n",
      " [2.80352123e-03]\n",
      " [1.24522755e-02]\n",
      " [8.66623282e-01]\n",
      " [2.34775990e-01]\n",
      " [1.79630294e-02]\n",
      " [9.50021960e-04]\n",
      " [1.46337701e-02]\n",
      " [1.31835848e-01]\n",
      " [7.19568282e-02]\n",
      " [2.55680203e-01]\n",
      " [5.88329369e-03]\n",
      " [7.85522570e-04]\n",
      " [7.35120499e-04]\n",
      " [2.08217418e-03]\n",
      " [4.77213878e-04]\n",
      " [5.10324026e-03]\n",
      " [6.03603432e-03]\n",
      " [4.43488881e-02]\n",
      " [7.25188255e-01]]\n"
     ]
    }
   ],
   "source": [
    "predss = mod_final.predict(test_xs, verbose=1)\n",
    "print(predss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137a1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.layers import deserialize as deserialize_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa5af75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassToken' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel-test1_0_epoch0-1.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, custom_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassToken\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mClassToken\u001b[49m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClassToken' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model-test1_0_epoch0-1.h5', custom_objects={\"ClassToken\": ClassToken})\n",
    "# loaded_model = load_model('bert_model.h5', custom_objects={\"TFBertModel\": transformers.TFBertModel})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cde64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import asarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd973adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('StenosisDetection/dataset/14_002_5_0016.bmp')\n",
    "numpydata = asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45210eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df939c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow69)",
   "language": "python",
   "name": "tensorflow69"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
